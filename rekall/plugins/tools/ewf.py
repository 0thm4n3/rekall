# Rekall Memory Forensics
#
# Copyright 2014 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
#

"""This file provides read/write support for EWF files.

EWF files are generated by Encase/FTK and are a common compressible storage
format for digital evidence.

The below code is based on libewf:
https://github.com/libyal/libewf
https://googledrive.com/host/0B3fBvzttpiiSMTdoaVExWWNsRjg/

"""

__author__ = "Michael Cohen <scudette@google.com>"
import array
import zlib

from rekall import utils
from rekall.plugins.overlays import basic


EWF_TYPES = dict(
    ewf_file_header_v1=[0x0d, {
        'EVF_sig': [0, ['Signature', dict(value="EVF\x09\x0d\x0a\xff\x00")]],

        'fields_start': [8, ['byte']],
        'segment_number': [9, ['unsigned short int']],
        'fields_end': [11, ['unsigned short int']],
        }],

    ewf_file_header_v2=[None, {
        'EVF_sig': [0, ['Signature', dict(value="EVF2\x0d\x0a\x81\x00")]],

        'major_version': [9, ['byte']],
        'minor_version': [10, ['byte']],

        'compression_method': [11, ['Enumeration', dict(
            target="unsigned short int",
            choices=dict(
                NONE=0,
                DEFLATE=1,
                BZIP2=2,
                )
            )]],
        'segment_number': [13, ['unsigned short int']],
        'set_identifier': [15, ['String', dict(length=16)]],
        }],

    ewf_section_descriptor_v1=[76, {
        # This string determines how to process this section.
        'type': [0, ['String', dict(length=16)]],

        # The next section in this file.
        'next': [16, ['Pointer', dict(
            target="ewf_section_descriptor_v1"
            )]],

        'size': [24, ['long long unsigned int']],
        'checksum': [72, ['unsigned int']],
        }],

    ewf_volume=[None, {
        'media_type': [0, ['Enumeration', dict(
            choices={
                0: 'remobable_disk',
                1: 'fixed_disk',
                2: 'optical_disk',
                3: 'LVF',
                4: 'memory',
                },
            )]],
        'number_of_chunks': [4, ['unsigned int']],
        'sectors_per_chunk': [8, ['unsigned int']],
        'bytes_per_sector': [12, ['unsigned int']],
        'number_of_sectors': [16, ['long long unsigned int']],
        'chs_cylinders': [24, ['unsigned int']],
        'chs_heads': [28, ['unsigned int']],
        'chs_sectors': [32, ['unsigned int']],

        'media_flags': [36, ['Flags', dict(
            maskmap={
                'image': 1,
                'physical': 2,
                'Fastblock Tableau write blocker': 4,
                'Tableau write blocker': 8
                })]],

        'compression_level': [52, ['Enumeration', dict(
            choices={
                0: 'no compression',
                1: 'fast/good compression',
                2: 'best compression',
                })]],
        }],

    ewf_table_entry=[4, {
        # Is the chunk compressed?
        'compressed': [0, ['BitField', dict(start_bit=31, end_bit=32)]],

        # The offset to the chunk within the file.
        'offset': [0, ['BitField', dict(start_bit=0, end_bit=31)]],
        }],

    ewf_table_header_v1=[lambda x: x.entries[x.number_of_entries].obj_end, {
        'number_of_entries': [0, ['long long unsigned int']],
        'base_offset': [8, ['long long unsigned int']],
        'checksum': [20, ['unsigned int']],

        # The table just contains a list of table entries to the start of each
        # chunk.
        'entries': [24, ['Array', dict(
            target='ewf_table_entry',
            count=lambda x: x.number_of_entries
            )]],
        }],

    )


class EWFProfile(basic.ProfileLLP64, basic.BasicClasses):
    """Basic profile for EWF files."""

    @classmethod
    def Initialize(cls, profile):
        super(EWFProfile, cls).Initialize(profile)

        profile.add_types(EWF_TYPES)


class EWFFile(object):
    """A helper for parsing an EWF file."""

    def __init__(self, session=None, address_space=None):
        self.session = session

        # This is a cache of tables. We can quickly find the table responsible
        # for a particular chunk.
        self.tables = utils.SortedCollection(key=lambda x: x[0])
        self._chunk_offset = 0
        self.chunk_size = 32 * 1024

        # 32kb * 100 = 3.2mb cache size.
        self.chunk_cache = utils.FastStore(max_size=100)

        self.address_space = address_space
        self.profile = EWFProfile(session=session)
        self.file_header = self.profile.ewf_file_header_v1(
            offset=0, vm=self.address_space)

        # Make sure the file signature is correct.
        if not self.file_header.EVF_sig.is_valid():
            raise RuntimeError("EVF signature does not match.")

        # Now locate all the sections in the file.
        first_section = self.profile.ewf_section_descriptor_v1(
            vm=self.address_space, offset=self.file_header.obj_end)

        for section in first_section.walk_list("next"):
            if section.type == "header2":
                self.handle_header2(section)

            elif section.type == "header":
                self.handle_header(section)

            elif section.type in ["disk", "volume"]:
                self.handle_volume(section)

            elif section.type == "table":
                self.handle_table(section)

        # How many chunks we actually have in this file.
        self.size = self._chunk_offset * self.chunk_size

    def handle_header(self, section):
        """Handle the header section.

        We do not currently do anything with it.
        """
        # The old header contains an ascii encoded description, compressed with
        # zlib.
        data = zlib.decompress(
            section.obj_vm.read(section.obj_end, section.size))

        # We dont do anything with this data right now.

    def handle_header2(self, section):
        """Handle the header2 section.

        We do not currently do anything with it.
        """
        # The header contains a utf16 encoded description, compressed with zlib.
        data = zlib.decompress(
            section.obj_vm.read(section.obj_end, section.size)).decode("utf16")

        # We dont do anything with this data right now.

    def handle_volume(self, section):
        """Handle the volume section.

        We mainly use it to know the chunk size.
        """
        volume_header = self.profile.ewf_volume(
            vm=self.address_space, offset=section.obj_end)

        self.chunk_size = (volume_header.sectors_per_chunk *
                           volume_header.bytes_per_sector)

    def handle_table(self, section):
        """Parse the table and store it in our lookup table."""
        table_header = self.profile.ewf_table_header_v1(
            vm=self.address_space, offset=section.obj_end)

        number_of_entries = table_header.number_of_entries

        # This is an optimization which allows us to avoid small reads for each
        # chunk. We just load the entire table into memory and read it on demand
        # from there.
        table = array.array("I")
        table.fromstring(self.address_space.read(
            table_header.entries.obj_offset,
            4 * table_header.number_of_entries))

        # We assume the last chunk is a full chunk. Feeding zlib.decompress()
        # extra data does not matter so we just read the most we can.
        table.append(table[-1] + self.chunk_size)

        self.tables.insert(
            # First chunk for this table, table header, table entry cache.
            (self._chunk_offset, table_header, table))

        # The next table starts at this chunk.
        self._chunk_offset += number_of_entries

    def read_chunk(self, chunk_id):
        """Read a single chunk from the file."""
        try:
            return self.chunk_cache.Get(chunk_id)
        except KeyError:
            start_chunk, table_header, table = self.tables.find_le(chunk_id)

            # This should be a ewf_table_entry object but the below is faster.
            table_entry = table[chunk_id - start_chunk]
            offset = table_entry & 0x7fffffff
            compressed_chunk_size = (
                table[chunk_id - start_chunk + 1] & 0x7fffffff - offset)

            data = self.address_space.read(
                offset + table_header.base_offset, compressed_chunk_size)

            if table_entry & 0x80000000:
                data = zlib.decompress(data)

            # Cache the chunk for later.
            self.chunk_cache.Put(chunk_id, data)

            return data

    def read_partial(self, offset, length):
        """Read as much as possible from the current offset."""
        # Find the table responsible for this chunk.
        chunk_id, chunk_offset = divmod(offset, self.chunk_size)
        available_length = min(length, self.chunk_size - chunk_offset)

        # Get the chunk and split it.
        data = self.read_chunk(chunk_id)

        return data[chunk_offset:chunk_offset + available_length]

    def read(self, offset, length):
        """Read data from the file."""
        # Most read operations are very short and will not need to merge chunks
        # at all. In that case concatenating strings is much faster than storing
        # partial reads into a list and join()ing them.
        result = ''
        available_length = length

        while available_length > 0:
            buf = self.read_partial(offset, available_length)
            if not buf:
                break

            result += buf
            offset += len(buf)
            available_length -= len(buf)

        return result
